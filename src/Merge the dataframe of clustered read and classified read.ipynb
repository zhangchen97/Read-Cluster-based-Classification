{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpath=\"/home/ubuntu/kraken2/database2/taxonomy/nodes.dmp\"\n",
    "#tmp_inpath=\"/home/ubuntu/kraken2res/tmp_nodes.dmp\"\n",
    "##get taxonomy's df，第一列是taxid,第二列是rank\n",
    "##input:filepath，output:dataframe\n",
    "def get_taxdf(inpath):\n",
    "    infile=open(inpath,'r')\n",
    "    rank=set()\n",
    "    nodedict=dict()\n",
    "    for line in infile:\n",
    "        line=line.strip()\n",
    "        arr=line.split(\"|\")\n",
    "        tmp_arr=[]\n",
    "        for item in arr:\n",
    "            tmp_arr.append(item.strip())\n",
    "        nodedict[int(tmp_arr[0])]=str(tmp_arr[2])\n",
    "        rank.add(tmp_arr[2])\n",
    "    infile.close()\n",
    "    print(rank)\n",
    "    taxser=pd.Series(nodedict)\n",
    "    taxdf=pd.DataFrame(taxser)\n",
    "    taxdf=taxdf.reset_index(drop=False)\n",
    "    taxdf.columns=['taxid','rank']\n",
    "    print(taxdf.head())\n",
    "    print(taxdf.shape)\n",
    "    return taxdf\n",
    "\n",
    "# inpath=\"/home/ubuntu/kraken2/compeleteres2.txt\"\n",
    "# labelpath=\"/home/ubuntu/mycami2/label50sample\"\n",
    "# get_kradf_cludf(inpath,labelpath)\n",
    "##输入1:kraken2 taxonomy result file path，\n",
    "##   2: reads的数量（注意，每个cluster的reads是连续的，而且数量一样多）,现在是50\n",
    "##output: dataframe,第一列是readname,第二列是kra_id（不能保证rank）\n",
    "def get_clark_df(inpath):\n",
    "    df = pd.read_csv(inpath, sep=\",\",usecols=[0,2])\n",
    "    df.columns=['readname','clark_id']\n",
    "    df.columns=['readname','clark_id']\n",
    "    df['clark_id'].fillna(0,inplace=True)\n",
    "    print(df.head())\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "#输入：1：raw read data(readname,seq)，\n",
    "#     2：cami2的label data file（readname,label），\n",
    "#     3：cami2 label2taxid mapping file(label,taxid)\n",
    "def get_rawdf(inpath,labelpath,metapath,clusterpath):\n",
    "    nameset=set()\n",
    "    count=0\n",
    "    infile=open(inpath,'r')\n",
    "    for line in infile:\n",
    "        if(count%2==0):\n",
    "            readname=line.strip().replace(\">\",\"\")\n",
    "            nameset.add(readname)\n",
    "        count=count+1\n",
    "    infile.close()\n",
    "    \n",
    "    ##读取答案文件\n",
    "    labelfile=open(labelpath,'r')\n",
    "    dict1=dict()\n",
    "    for line in labelfile:\n",
    "        arr=line.strip().split(\"\\t\")\n",
    "        if arr[0] in nameset:\n",
    "            dict1[arr[0]]=arr[1]\n",
    "    labelfile.close()\n",
    "    ser=pd.Series(dict1)\n",
    "    df=pd.DataFrame(ser,index=None)\n",
    "    df=df.reset_index(drop=False)\n",
    "    df.columns=['readname','labelid']\n",
    "\n",
    "    ##读取label到taxid的映射\n",
    "    meta = pd.read_csv(metapath, sep=\"\\t\",usecols=[0,2])\n",
    "    meta.columns=['labelid','taxid']\n",
    "    df=pd.merge(df,meta,how='inner',on='labelid')\n",
    "    \n",
    "    ##读取local clustering到read的映射\n",
    "    clu_df = pd.read_csv(clusterpath, sep=\"\\t\",header=None)\n",
    "    clu_df.columns=['readname','clusterid']\n",
    "    df=pd.merge(df,clu_df,how='inner',on='readname')\n",
    "    df=df.sort_values(by=['readname'])\n",
    "    print(\"=======================================打印前五行===================================================\")\n",
    "    print(df.head())\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "inpath=\"/home/ubuntu/kraken2/database2/taxonomy/nodes.dmp\"\n",
    "tmp_inpath=\"/home/ubuntu/kraken2res/tmp_nodes.dmp\"\n",
    "infile=open(inpath,'r')\n",
    "rank=set()\n",
    "nodedict=dict()\n",
    "for line in infile:\n",
    "    line=line.strip()\n",
    "    arr=line.split(\"|\")\n",
    "    tmp_arr=[]\n",
    "    for item in arr:\n",
    "        tmp_arr.append(item.strip())\n",
    "    #print(tmp_arr)\n",
    "    nodedict[int(tmp_arr[0])]=[int(tmp_arr[1]),str(tmp_arr[2])]\n",
    "    rank.add(tmp_arr[2])\n",
    "infile.close()\n",
    "\n",
    "## level选择species,genus, Obtain the taxid corresponding to the level\n",
    "def get_level(taxid,level):\n",
    "    if(taxid==0):  ##未分类\n",
    "        return 0\n",
    "    if taxid not in nodedict:\n",
    "        return -1\n",
    "    rank=nodedict[taxid]\n",
    "    parent_taxid=rank[0]\n",
    "    rank_name=rank[1]\n",
    "\n",
    "    if(rank_name==level): ##本身就是level级别的\n",
    "        return taxid\n",
    "    \n",
    "    above_level={\"kingdom\",\"phylum\",\"class\",\"family\",\"order\"}\n",
    "    if(level==\"species\"): \n",
    "        above_level.add(\"genus\")\n",
    "\n",
    "    while( rank_name not in above_level):\n",
    "        rank=nodedict[parent_taxid]\n",
    "        rank_name=rank[1]\n",
    "        #print(rank)\n",
    "        if(rank_name==level):\n",
    "            return parent_taxid\n",
    "        if(parent_taxid==rank[0]): ##还等于上一次循环的taxid，因此来避免死循环\n",
    "            return -1\n",
    "        parent_taxid=rank[0]\n",
    "    if(rank_name!=level):\n",
    "        return -1\n",
    "\n",
    "def get_mergealldf(df,taxdf,kra_df):\n",
    "    #产生答案的df\n",
    "    new_df=pd.merge(df,taxdf,how='left',on='taxid')\n",
    "    new_df.rename(columns = {\"rank\": \"true_rank\",\"taxid\": \"true_taxid\"},  inplace=True)\n",
    "    print(\"all the rank of raw reads result:  \")\n",
    "    print(set(list(new_df['true_rank'])))\n",
    "\n",
    "    ##合并kraken2预测值\n",
    "    new_df2=pd.merge(new_df,kra_df,how='left',on='readname')\n",
    "    new_df2=pd.merge(new_df2,taxdf,how='left',left_on='clark_id',right_on='taxid')\n",
    "    new_df2=new_df2.drop(['taxid'],axis=1)\n",
    "    new_df2.rename(columns = {\"rank\": \"clark_rank\"},  inplace=True)\n",
    "    print(\"all the rank of clark result: \")\n",
    "    print(set(list(new_df2['clark_rank'])))\n",
    "    print(new_df2.head())\n",
    "    return new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================打印前五行===================================================\n",
      "              readname    labelid    taxid  clusterid\n",
      "3514516   S13R10000006   851733.0  1714682     287052\n",
      "49178639  S13R10000016   182764.0     1599    1841065\n",
      "58319923  S13R10000026   463794.1     1587    1669071\n",
      "18544879  S13R10000036  4378740.1   873513     587753\n",
      "28407027  S13R10000046  4354103.0    89059     353077\n",
      "(82279711, 4)\n"
     ]
    }
   ],
   "source": [
    "rawpath=\"/home/ubuntu/CAMI2_Real_Test/allReads_dir/CAMI2_fasta_50Sample.fa\"\n",
    "clusterpath=\"/home/ubuntu/LocalClustering_Result_2021_8_zc/ccaddseq_local_nofilter.csv\"\n",
    "labelpath=\"/home/ubuntu/mycami2/label50sample\"\n",
    "metapath=\"/home/ubuntu/mycami2/metadata.tsv\"\n",
    "df=get_rawdf(rawpath,labelpath,metapath,clusterpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarkpath=\"/home/ubuntu/clark/data/CAMI2_Allreads_clark_result.txt.csv\"\n",
    "clark_df=get_clark_df(clarkpath)\n",
    "dataframe=get_mergealldf(df,taxdf,clark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get species level Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_species_df(df):\n",
    "    new_df=df.copy(deep=True)\n",
    "    #统计species级别\n",
    "    level=\"species\"\n",
    "    new_df['true_species']=new_df[\"true_taxid\"].apply(lambda x: get_level(x,level))\n",
    "    new_df['clark_species']=new_df[\"clark_id\"].apply(lambda x: get_level(x,level))\n",
    "    new_df.insert(1,'clu_species',1)\n",
    "    new_df=new_df.set_index('clusterid')\n",
    "    print(new_df.head())\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               readname  clu_species    labelid  true_taxid true_rank  \\\n",
      "clusterid                                                               \n",
      "287052     S13R10000006            1   851733.0     1714682   species   \n",
      "1841065    S13R10000016            1   182764.0        1599   species   \n",
      "1669071    S13R10000026            1   463794.1        1587   species   \n",
      "587753     S13R10000036            1  4378740.1      873513    strain   \n",
      "353077     S13R10000046            1  4354103.0       89059   species   \n",
      "\n",
      "           clark_id clark_rank  true_species  clark_species  \n",
      "clusterid                                                    \n",
      "287052     114090.0    species       1714682       114090.0  \n",
      "1841065      1599.0    species          1599         1599.0  \n",
      "1669071      1587.0    species          1587         1587.0  \n",
      "587753          0.0        NaN         28126            0.0  \n",
      "353077      89059.0    species         89059        89059.0  \n"
     ]
    }
   ],
   "source": [
    "species_df=get_species_df(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the dataframe , then use the databrick to group by clusterid to get the taxid of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "species_df.to_csv(\"/home/ubuntu/clark/data/CAMI2_mouse_clark_tax_df.csv\")  \n",
    "print(\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
